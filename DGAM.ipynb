{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LqVAQo9oPME"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Gamma Correction\n"
      ],
      "metadata": {
        "id": "bZx6S2y6oB8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_argmax(x, beta=100.0):\n",
        "    \"\"\"\n",
        "    Differentiable approximation of argmax using softmax with temperature scaling.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor of shape [batch_size, num_experts]\n",
        "        beta: Temperature parameter (higher values make it closer to argmax)\n",
        "\n",
        "    Returns:\n",
        "        Soft argmax values of shape [batch_size]\n",
        "    \"\"\"\n",
        "    # Apply temperature scaling for sharper distribution\n",
        "    scaled_x = beta * x\n",
        "    # Compute softmax probabilities\n",
        "    softmax_probs = F.softmax(scaled_x, dim=-1)\n",
        "    # Create indices tensor\n",
        "    indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "    # Compute weighted sum of indices\n",
        "    soft_indices = torch.sum(indices * softmax_probs, dim=-1)\n",
        "\n",
        "    return soft_indices, softmax_probs\n"
      ],
      "metadata": {
        "id": "SZFk9a_ytbeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Extract statistical features from video clips for AGC\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    def forward(self, video_clip):\n",
        "        \"\"\"\n",
        "        Extract mean, std, and Shannon entropy for each frame\n",
        "\n",
        "        Args:\n",
        "            video_clip: Tensor of shape [batch_size, T, H, W, C]\n",
        "\n",
        "        Returns:\n",
        "            features: Tensor of shape [batch_size, T*3]\n",
        "        \"\"\"\n",
        "        batch_size, T, H, W, C = video_clip.shape\n",
        "        features = []\n",
        "\n",
        "        for t in range(T):\n",
        "            frame = video_clip[:, t]  # [batch_size, H, W, C]\n",
        "\n",
        "            # Convert to grayscale if needed\n",
        "            if C == 3:\n",
        "                gray_frame = torch.mean(frame, dim=-1)  # [batch_size, H, W]\n",
        "            else:\n",
        "                gray_frame = frame.squeeze(-1)\n",
        "\n",
        "            # Extract statistical features\n",
        "            mean_val = torch.mean(gray_frame, dim=(1, 2))  # [batch_size]\n",
        "            std_val = torch.std(gray_frame, dim=(1, 2))    # [batch_size]\n",
        "\n",
        "            # Calculate Shannon entropy\n",
        "            entropy_val = self._calculate_entropy(gray_frame)  # [batch_size]\n",
        "\n",
        "            # Stack features for this frame\n",
        "            frame_features = torch.stack([mean_val, std_val, entropy_val], dim=1)  # [batch_size, 3]\n",
        "            features.append(frame_features)\n",
        "\n",
        "        # Concatenate all frame features\n",
        "        features = torch.cat(features, dim=1)  # [batch_size, T*3]\n",
        "        return features\n",
        "\n",
        "    def _calculate_entropy(self, gray_frame):\n",
        "        \"\"\"Calculate Shannon entropy for grayscale frames\"\"\"\n",
        "        batch_size = gray_frame.shape[0]\n",
        "        entropies = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            frame = gray_frame[i].flatten()\n",
        "            # Create histogram\n",
        "            hist = torch.histc(frame, bins=256, min=0.0, max=1.0)\n",
        "            # Normalize to get probabilities\n",
        "            hist = hist / hist.sum()\n",
        "            # Remove zeros to avoid log(0)\n",
        "            hist = hist[hist > 0]\n",
        "            # Calculate entropy\n",
        "            entropy = -torch.sum(hist * torch.log2(hist))\n",
        "            entropies.append(entropy)\n",
        "\n",
        "        return torch.stack(entropies)\n"
      ],
      "metadata": {
        "id": "Oadn1G7ftdTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingNetwork(nn.Module):\n",
        "    \"\"\"Gating network that uses SoftArgmax for differentiable expert selection\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, num_experts=5, hidden_dim=64, beta=100.0):\n",
        "        super(GatingNetwork, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.beta = beta\n",
        "\n",
        "        # Three-layer fully connected network with batch normalization\n",
        "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, num_experts)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through gating network\n",
        "\n",
        "        Args:\n",
        "            x: Feature tensor of shape [batch_size, input_size]\n",
        "\n",
        "        Returns:\n",
        "            lambda_vector: Expert weights [batch_size, num_experts]\n",
        "            gate_indices: Soft argmax indices [batch_size]\n",
        "        \"\"\"\n",
        "        # First layer: FC1 -> BatchNorm1 -> ReLU\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Second layer: FC2 -> BatchNorm2 -> ReLU\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Output layer: FC3\n",
        "        logits = self.fc3(x)  # [batch_size, num_experts]\n",
        "\n",
        "        # Apply SoftArgmax to get differentiable indices\n",
        "        gate_indices, lambda_vector = soft_argmax(logits, self.beta)\n",
        "\n",
        "        return lambda_vector, gate_indices\n"
      ],
      "metadata": {
        "id": "YoV98EvptfIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GammaIntensityCorrection(nn.Module):\n",
        "    \"\"\"Gamma Intensity Correction module with multiple gamma values\"\"\"\n",
        "\n",
        "    def __init__(self, gamma_values=[1.0, 1.5, 2.0, 2.5, 3.0]):\n",
        "        super(GammaIntensityCorrection, self).__init__()\n",
        "        self.gamma_values = gamma_values\n",
        "        self.num_experts = len(gamma_values)\n",
        "\n",
        "    def forward(self, video_clip, expert_weights):\n",
        "        \"\"\"\n",
        "        Apply weighted gamma correction based on expert selection\n",
        "\n",
        "        Args:\n",
        "            video_clip: Input video [batch_size, T, H, W, C]\n",
        "            expert_weights: Gating weights [batch_size, num_experts]\n",
        "\n",
        "        Returns:\n",
        "            corrected_clip: Gamma corrected video [batch_size, T, H, W, C]\n",
        "        \"\"\"\n",
        "        batch_size = video_clip.shape[0]\n",
        "        corrected_clips = []\n",
        "\n",
        "        # Apply each gamma correction\n",
        "        for i, gamma in enumerate(self.gamma_values):\n",
        "            gamma_corrected = self._apply_gamma_correction(video_clip, gamma)\n",
        "            corrected_clips.append(gamma_corrected)\n",
        "\n",
        "        # Stack all corrected clips\n",
        "        corrected_clips = torch.stack(corrected_clips, dim=1)  # [batch_size, num_experts, T, H, W, C]\n",
        "\n",
        "        # Weighted combination using expert weights\n",
        "        expert_weights = expert_weights.view(batch_size, self.num_experts, 1, 1, 1, 1)\n",
        "        final_clip = torch.sum(corrected_clips * expert_weights, dim=1)  # [batch_size, T, H, W, C]\n",
        "\n",
        "        return final_clip\n",
        "\n",
        "    def _apply_gamma_correction(self, image, gamma):\n",
        "        \"\"\"\n",
        "        Apply gamma intensity correction formula:\n",
        "        GIC_gamma(I) = [(max(I) - min(I)) * ((I - min(I))/(max(I) - min(I)))^(1/gamma)] + min(I)\n",
        "        \"\"\"\n",
        "        # Ensure image is in float format\n",
        "        if image.dtype != torch.float32:\n",
        "            image = image.float()\n",
        "\n",
        "        # Normalize to [0,1] if needed\n",
        "        if torch.max(image) > 1.0:\n",
        "            image = image / 255.0\n",
        "\n",
        "        # Apply GIC formula per batch\n",
        "        batch_size = image.shape[0]\n",
        "        corrected_images = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            img = image[i]\n",
        "            min_val = torch.min(img)\n",
        "            max_val = torch.max(img)\n",
        "\n",
        "            if max_val > min_val:  # Avoid division by zero\n",
        "                normalized = (img - min_val) / (max_val - min_val)\n",
        "                corrected = torch.pow(normalized, 1.0/gamma)\n",
        "                result = (max_val - min_val) * corrected + min_val\n",
        "            else:\n",
        "                result = img\n",
        "\n",
        "            corrected_images.append(result)\n",
        "\n",
        "        return torch.stack(corrected_images, dim=0)\n"
      ],
      "metadata": {
        "id": "e_ynwNuxtg0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveGammaCorrection(nn.Module):\n",
        "    \"\"\"Complete Adaptive Gamma Correction module using SoftArgmax\"\"\"\n",
        "\n",
        "    def __init__(self, gamma_values=[1.0, 1.5, 2.0, 2.5, 3.0], hidden_dim=64, beta=100.0):\n",
        "        super(AdaptiveGammaCorrection, self).__init__()\n",
        "        self.gamma_values = gamma_values\n",
        "        self.num_experts = len(gamma_values)\n",
        "\n",
        "        # Feature extractor\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "\n",
        "        # Gating network (will be initialized after seeing first input)\n",
        "        self.gating_network = None\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        # Gamma correction module\n",
        "        self.gic = GammaIntensityCorrection(gamma_values)\n",
        "\n",
        "    def forward(self, video_clip):\n",
        "        \"\"\"\n",
        "        Process video clip with adaptive gamma correction\n",
        "\n",
        "        Args:\n",
        "            video_clip: Input video tensor [batch_size, T, H, W, C]\n",
        "\n",
        "        Returns:\n",
        "            corrected_clip: Gamma corrected video [batch_size, T, H, W, C]\n",
        "            lambda_vector: Expert weights [batch_size, num_experts]\n",
        "            gate_indices: Selected expert indices [batch_size]\n",
        "            selected_gammas: Selected gamma values [batch_size]\n",
        "        \"\"\"\n",
        "        # Extract features\n",
        "        features = self.feature_extractor(video_clip)  # [batch_size, T*3]\n",
        "\n",
        "        # Initialize gating network if needed\n",
        "        if self.gating_network is None:\n",
        "            input_size = features.shape[1]\n",
        "            self.gating_network = GatingNetwork(\n",
        "                input_size, self.num_experts, self.hidden_dim, self.beta\n",
        "            ).to(video_clip.device)\n",
        "\n",
        "        # Get expert weights and indices using SoftArgmax\n",
        "        lambda_vector, gate_indices = self.gating_network(features)\n",
        "\n",
        "        # Apply gamma correction\n",
        "        corrected_clip = self.gic(video_clip, lambda_vector)\n",
        "\n",
        "        # Get selected gamma values for interpretation\n",
        "        selected_gammas = self._get_selected_gammas(gate_indices)\n",
        "\n",
        "        return corrected_clip, lambda_vector, gate_indices, selected_gammas\n",
        "\n",
        "    def _get_selected_gammas(self, gate_indices):\n",
        "        \"\"\"Convert soft indices to gamma values for interpretation\"\"\"\n",
        "        gamma_tensor = torch.tensor(self.gamma_values, device=gate_indices.device)\n",
        "        # Use the soft indices to interpolate between gamma values\n",
        "        indices_clamped = torch.clamp(gate_indices, 0, len(self.gamma_values) - 1)\n",
        "\n",
        "        # Linear interpolation between adjacent gamma values\n",
        "        lower_idx = torch.floor(indices_clamped).long()\n",
        "        upper_idx = torch.clamp(lower_idx + 1, 0, len(self.gamma_values) - 1)\n",
        "\n",
        "        alpha = indices_clamped - lower_idx.float()\n",
        "        selected_gammas = (1 - alpha) * gamma_tensor[lower_idx] + alpha * gamma_tensor[upper_idx]\n",
        "\n",
        "        return selected_gammas\n"
      ],
      "metadata": {
        "id": "zPfLihEMtjh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agc = AdaptiveGammaCorrection(\n",
        "    gamma_values=[1.0, 1.5, 2.0, 2.5, 3.0],\n",
        "    hidden_dim=64,\n",
        "    beta=100.0\n",
        ")"
      ],
      "metadata": {
        "id": "5FItULjXtlv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_clip = torch.randn(2, 32, 224, 224, 3)"
      ],
      "metadata": {
        "id": "C5pA11c2t0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_clip, lambda_vector, gate_indices, selected_gammas = agc(video_clip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW19ss2ut2li",
        "outputId": "c6a1c534-958b-4433-e474-01b3315efedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: torch.Size([2, 32, 224, 224, 3])\n",
            "Corrected shape: torch.Size([2, 32, 224, 224, 3])\n",
            "Expert weights: tensor([[1.0000e+00, 2.6171e-30, 1.5817e-09, 3.6904e-30, 0.0000e+00],\n",
            "        [8.2547e-09, 9.2468e-14, 1.0717e-05, 6.2197e-01, 3.7802e-01]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Gate indices: tensor([3.1633e-09, 3.3780e+00], grad_fn=<SumBackward1>)\n",
            "Selected gammas: tensor([1.0000, 2.6890], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_clip.shape, lambda_vector.shape, gate_indices.shape, selected_gammas.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDm-mIAht44X",
        "outputId": "8320031d-1c0f-4c75-9d2e-c062f6a56939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 32, 224, 224, 3]),\n",
              " torch.Size([2, 5]),\n",
              " torch.Size([2]),\n",
              " torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Swin Backbone"
      ],
      "metadata": {
        "id": "epz-9s4noHni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.video import swin3d_s,Swin3D_S_Weights"
      ],
      "metadata": {
        "id": "ExKsp_8bvFVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoSwinBackbone(nn.Module):\n",
        "  def __init__(self,pretrained=True):\n",
        "    super().__init__()\n",
        "    weights = Swin3D_S_Weights.KINETICS400_V1 if pretrained else None\n",
        "    self.model = swin3d_s(weights=weights)\n",
        "\n",
        "  def forward(self,x): # X -> [B,T,H,W,C]\n",
        "    x = x.permute(0,4,1,2,3) # [B,C,T,H,W]\n",
        "    x = self.model.patch_embed(x) # B _T _H _W C\n",
        "    x = self.model.pos_drop(x)\n",
        "    x = self.model.features(x)  # B _T _H _W C\n",
        "    x = self.model.norm(x)\n",
        "    x = x.permute(0, 4, 1, 2, 3)  # B, C, _T, _H, _W\n",
        "    return x"
      ],
      "metadata": {
        "id": "UL1RPHXju8ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = VideoSwinBackbone()(corrected_clip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9npua_g0UTl",
        "outputId": "9001b632-3d3e-4902-ac10-17bc52cc89c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/swin3d_s-da41c237.pth\" to /root/.cache/torch/hub/checkpoints/swin3d_s-da41c237.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218M/218M [00:02<00:00, 85.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2TvNVWl0krM",
        "outputId": "10e9289d-8e21-4213-9435-f75514cb9caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 768, 16, 7, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Head  Selection"
      ],
      "metadata": {
        "id": "19YiZNH8oPm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class I3DHead(nn.Module):\n",
        "    \"\"\"\n",
        "    I3Dâ€style classification head:\n",
        "      - AdaptiveAvgPool3d â†’ Flatten â†’ Dropout â†’ Linear\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_classes, dropout_rate=0.5):\n",
        "        super(I3DHead, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool3d(1)            # [B, C, 1,1,1]\n",
        "        self.dropout = nn.Dropout(dropout_rate)        # regularization\n",
        "        self.fc = nn.Linear(in_channels, num_classes)  # final classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [B, C, T', H', W']\n",
        "        returns: [B, num_classes]\n",
        "        \"\"\"\n",
        "        x = self.pool(x)        # [B, C, 1, 1, 1]\n",
        "        x = torch.flatten(x, 1) # [B, C]\n",
        "        x = self.dropout(x)     # [B, C]\n",
        "        return self.fc(x)       # [B, num_classes]\n"
      ],
      "metadata": {
        "id": "UUAjGi-F07Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveHeadSelection(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Head Selection module:\n",
        "      - Holds one I3DHead per expert\n",
        "      - Aggregates head outputs using gating weights\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_classes, num_experts, dropout_rate=0.5):\n",
        "        super(AdaptiveHeadSelection, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.heads = nn.ModuleList([\n",
        "            I3DHead(in_channels, num_classes, dropout_rate)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, features, expert_weights):\n",
        "        \"\"\"\n",
        "        features: [B, C, T', H', W']\n",
        "        expert_weights: [B, E] from AGC gating (softmax probabilities)\n",
        "        returns:\n",
        "          - logits: [B, num_classes]\n",
        "          - per_head_logits: [B, E, num_classes]\n",
        "        \"\"\"\n",
        "        # Compute logits for each expert head\n",
        "        head_logits = []\n",
        "        for head in self.heads:\n",
        "            head_logits.append(head(features).unsqueeze(1))  # [B,1,num_classes]\n",
        "        head_logits = torch.cat(head_logits, dim=1)         # [B,E,num_classes]\n",
        "\n",
        "        # Weight and sum logits over experts\n",
        "        weights = expert_weights.unsqueeze(-1)              # [B,E,1]\n",
        "        fused_logits = torch.sum(weights * head_logits, dim=1)  # [B,num_classes]\n",
        "        return fused_logits, head_logits"
      ],
      "metadata": {
        "id": "jPzt465wCNGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined"
      ],
      "metadata": {
        "id": "5BJOXlmjoTqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGAMModel(nn.Module):\n",
        "    def __init__(self, agc, backbone, num_classes):\n",
        "        super(DGAMModel, self).__init__()\n",
        "        self.agc = agc                           # AdaptiveGammaCorrection\n",
        "        self.backbone = backbone                 # VideoSwinBackbone\n",
        "        self.ahs = AdaptiveHeadSelection(\n",
        "            in_channels=backbone.model.num_features,\n",
        "            num_classes=num_classes,\n",
        "            num_experts=agc.num_experts\n",
        "        )\n",
        "\n",
        "    def forward(self, video_clip):\n",
        "        \"\"\"\n",
        "        Returns final class logits and auxiliary perâ€expert logits.\n",
        "        \"\"\"\n",
        "        # 1. Adaptive gamma correction\n",
        "        corrected_clip, expert_weights, _, _ = self.agc(video_clip)\n",
        "        # 2. Feature extraction\n",
        "        features = self.backbone(corrected_clip)  # [B,C,T',H',W']\n",
        "        # 3. Adaptive head selection\n",
        "        fused_logits, per_head_logits = self.ahs(features, expert_weights)\n",
        "        return fused_logits, per_head_logits"
      ],
      "metadata": {
        "id": "ntEaQLYzCQfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = VideoSwinBackbone(pretrained=True)  # or True if pretrained weights are needed\n",
        "model = DGAMModel(agc=agc, backbone=backbone, num_classes=2)\n",
        "\n",
        "# Run forward pass\n",
        "output = model(torch.randn(2, 32, 224, 224, 3))"
      ],
      "metadata": {
        "id": "XU5hzpGyCUlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "723JKWKkKNZa",
        "outputId": "f5e39b35-124e-4121-e84b-6626b64ae4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0940, -0.0656],\n",
              "         [-0.0791, -0.1593]], grad_fn=<SumBackward1>),\n",
              " tensor([[[-0.0940, -0.0656],\n",
              "          [ 0.0873, -0.1315],\n",
              "          [ 0.0351,  0.1086],\n",
              "          [-0.0880, -0.0831],\n",
              "          [ 0.0607,  0.0510]],\n",
              " \n",
              "         [[-0.1210, -0.0157],\n",
              "          [ 0.0921, -0.4061],\n",
              "          [ 0.0368,  0.1223],\n",
              "          [-0.0791, -0.1593],\n",
              "          [-0.0153, -0.1523]]], grad_fn=<CatBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[0].shape, output[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRDog8hmL1Yd",
        "outputId": "36d2c147-8306-41e9-9817-6f3ea2af4bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 2]), torch.Size([2, 5, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from torch.optim import AdamW\n",
        "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# def get_optimizer(model):\n",
        "#     # Custom LR for gating\n",
        "#     params = [\n",
        "#         {\"params\": [], \"lr\": 3e-4},  # default group\n",
        "#         {\"params\": [], \"lr\": 3e-4 * 1000}  # gating group\n",
        "#     ]\n",
        "\n",
        "#     for name, param in model.named_parameters():\n",
        "#         if not param.requires_grad:\n",
        "#             continue\n",
        "#         if \"gating_network\" in name:\n",
        "#             params[1][\"params\"].append(param)\n",
        "#         else:\n",
        "#             params[0][\"params\"].append(param)\n",
        "\n",
        "#     return AdamW(params, betas=(0.9, 0.999), weight_decay=0.05)\n",
        "\n",
        "# def accuracy(output, target, topk=(1, 5)):\n",
        "#     \"\"\"Computes the top-k accuracy\"\"\"\n",
        "#     maxk = max(topk)\n",
        "#     _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)\n",
        "#     correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
        "\n",
        "#     return [\n",
        "#         correct[:, :k].float().sum().item() / target.size(0)\n",
        "#         for k in topk\n",
        "#     ]\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def evaluate(model, dataloader, device):\n",
        "#     model.eval()\n",
        "#     loss_total, correct1, correct5, total = 0.0, 0, 0, 0\n",
        "\n",
        "#     for video_clip, labels in dataloader:\n",
        "#         video_clip, labels = video_clip.to(device), labels.to(device)\n",
        "\n",
        "#         with autocast():\n",
        "#             fused_logits, _ = model(video_clip)\n",
        "#             loss = F.cross_entropy(fused_logits, labels)\n",
        "\n",
        "#         loss_total += loss.item() * labels.size(0)\n",
        "#         acc1, acc5 = accuracy(fused_logits, labels)\n",
        "#         correct1 += acc1 * labels.size(0)\n",
        "#         correct5 += acc5 * labels.size(0)\n",
        "#         total += labels.size(0)\n",
        "\n",
        "#     return loss_total / total, correct1 / total, correct5 / total\n",
        "\n",
        "# def train_model(model, train_loader, val_loader, device):\n",
        "#     total_epochs = 100\n",
        "#     warmup_epochs = 2.5\n",
        "#     warmup_steps = int(warmup_epochs * len(train_loader))\n",
        "#     global_step = 0\n",
        "\n",
        "#     model.to(device)\n",
        "#     optimizer = get_optimizer(model)\n",
        "#     scaler = GradScaler()\n",
        "#     scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs - int(warmup_epochs))\n",
        "\n",
        "#     for epoch in range(1, total_epochs + 1):\n",
        "#         model.train()\n",
        "#         train_loss, correct1, correct5, total = 0.0, 0, 0, 0\n",
        "\n",
        "#         for video_clip, labels in train_loader:\n",
        "#             video_clip, labels = video_clip.to(device), labels.to(device)\n",
        "#             global_step += 1\n",
        "\n",
        "#             if epoch <= warmup_epochs:\n",
        "#                 lr_scale = global_step / warmup_steps\n",
        "#                 for pg in optimizer.param_groups:\n",
        "#                     pg[\"lr\"] = 3e-4 * (1000 if pg[\"lr\"] > 3e-4 else 1) * lr_scale\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             with autocast():\n",
        "#                 fused_logits, _ = model(video_clip)\n",
        "#                 loss = F.cross_entropy(fused_logits, labels)\n",
        "\n",
        "#             scaler.scale(loss).backward()\n",
        "#             scaler.step(optimizer)\n",
        "#             scaler.update()\n",
        "\n",
        "#             train_loss += loss.item() * labels.size(0)\n",
        "#             acc1, acc5 = accuracy(fused_logits, labels)\n",
        "#             correct1 += acc1 * labels.size(0)\n",
        "#             correct5 += acc5 * labels.size(0)\n",
        "#             total += labels.size(0)\n",
        "\n",
        "#         if epoch > warmup_epochs:\n",
        "#             scheduler.step()\n",
        "\n",
        "#         print(f\"[Epoch {epoch:03d}] Train Loss: {train_loss/total:.4f}, \"\n",
        "#               f\"Top-1: {correct1/total:.4f}, Top-5: {correct5/total:.4f}\")\n",
        "\n",
        "#         # Evaluate every 2 epochs\n",
        "#         if epoch % 2 == 0:\n",
        "#             val_loss, val_top1, val_top5 = evaluate(model, val_loader, device)\n",
        "#             print(f\"[Epoch {epoch:03d}] Val Loss: {val_loss:.4f}, \"\n",
        "#                   f\"Top-1: {val_top1:.4f}, Top-5: {val_top5:.4f}\")\n"
      ],
      "metadata": {
        "id": "Oe1p2tO-MIad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1. Freeze utilities\n",
        "# ------------------------------------------\n",
        "def freeze_module(module):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "def unfreeze_module(module):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "def freeze_backbone_and_heads(model):\n",
        "    freeze_module(model.backbone)\n",
        "    for head in model.ahs.heads:\n",
        "        freeze_module(head)\n",
        "\n",
        "def freeze_gating(model):\n",
        "    freeze_module(model.agc.gating_network)\n",
        "\n",
        "def unfreeze_backbone_and_heads(model):\n",
        "    unfreeze_module(model.backbone)\n",
        "    for head in model.ahs.heads:\n",
        "        unfreeze_module(head)\n",
        "\n",
        "def unfreeze_gating(model):\n",
        "    unfreeze_module(model.agc.gating_network)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2. Optimizer builder\n",
        "# ------------------------------------------\n",
        "def get_optimizer(model, gating_lr_mult=1.0):\n",
        "    base_params, gating_params = [], []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"gating_network\" in name:\n",
        "            gating_params.append(param)\n",
        "        else:\n",
        "            base_params.append(param)\n",
        "\n",
        "    return AdamW([\n",
        "        {\"params\": base_params, \"lr\": 3e-4},\n",
        "        {\"params\": gating_params, \"lr\": 3e-4 * gating_lr_mult}\n",
        "    ], betas=(0.9, 0.999), weight_decay=0.05)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3. Accuracy metrics\n",
        "# ------------------------------------------\n",
        "def accuracy(output, target, topk=(1, 5)):\n",
        "    maxk = max(topk)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
        "    return [correct[:, :k].float().sum().item() / target.size(0) for k in topk]\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4. Evaluation\n",
        "# ------------------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    loss_sum, correct1, correct5, total = 0.0, 0, 0, 0\n",
        "\n",
        "    for video_clip, labels in dataloader:\n",
        "        video_clip, labels = video_clip.to(device), labels.to(device)\n",
        "        with autocast():\n",
        "            fused_logits, _ = model(video_clip)\n",
        "            loss = F.cross_entropy(fused_logits, labels)\n",
        "\n",
        "        loss_sum += loss.item() * labels.size(0)\n",
        "        acc1, acc5 = accuracy(fused_logits, labels)\n",
        "        correct1 += acc1 * labels.size(0)\n",
        "        correct5 += acc5 * labels.size(0)\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return loss_sum / total, correct1 / total, correct5 / total\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5. Full training loop with switching logic\n",
        "# ------------------------------------------\n",
        "def train_dgam_switching(model, train_loader, val_loader, device,\n",
        "                         stage1_epochs=30, stage2_epochs=30):\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # -------------------------------\n",
        "    # Stage 1: Train backbone + heads\n",
        "    # -------------------------------\n",
        "    print(\"ðŸ§  Stage 1: Training backbone and heads (gating frozen)\")\n",
        "    freeze_gating(model)\n",
        "    unfreeze_backbone_and_heads(model)\n",
        "\n",
        "    optimizer = get_optimizer(model, gating_lr_mult=1.0)  # gating ignored\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=stage1_epochs)\n",
        "\n",
        "    for epoch in range(1, stage1_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, correct1, correct5, total = 0.0, 0, 0, 0\n",
        "\n",
        "        for video_clip, labels in train_loader:\n",
        "            video_clip, labels = video_clip.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                fused_logits, _ = model(video_clip)\n",
        "                loss = F.cross_entropy(fused_logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            acc1, acc5 = accuracy(fused_logits, labels)\n",
        "            correct1 += acc1 * labels.size(0)\n",
        "            correct5 += acc5 * labels.size(0)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"[Stage1-Epoch {epoch:02d}] Loss: {total_loss/total:.4f}, \"\n",
        "              f\"Top-1: {correct1/total:.4f}, Top-5: {correct5/total:.4f}\")\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            val_loss, val_top1, val_top5 = evaluate(model, val_loader, device)\n",
        "            print(f\"[Stage1-Epoch {epoch:02d}] Val Loss: {val_loss:.4f}, \"\n",
        "                  f\"Top-1: {val_top1:.4f}, Top-5: {val_top5:.4f}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Stage 2: Train gating only\n",
        "    # -------------------------------\n",
        "    print(\"ðŸ”€ Stage 2: Training gating network (backbone & heads frozen)\")\n",
        "    freeze_backbone_and_heads(model)\n",
        "    unfreeze_gating(model)\n",
        "\n",
        "    optimizer = get_optimizer(model, gating_lr_mult=1000.0)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=stage2_epochs)\n",
        "\n",
        "    for epoch in range(1, stage2_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, correct1, correct5, total = 0.0, 0, 0, 0\n",
        "\n",
        "        for video_clip, labels in train_loader:\n",
        "            video_clip, labels = video_clip.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                fused_logits, _ = model(video_clip)\n",
        "                loss = F.cross_entropy(fused_logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            acc1, acc5 = accuracy(fused_logits, labels)\n",
        "            correct1 += acc1 * labels.size(0)\n",
        "            correct5 += acc5 * labels.size(0)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"[Stage2-Epoch {epoch:02d}] Loss: {total_loss/total:.4f}, \"\n",
        "              f\"Top-1: {correct1/total:.4f}, Top-5: {correct5/total:.4f}\")\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            val_loss, val_top1, val_top5 = evaluate(model, val_loader, device)\n",
        "            print(f\"[Stage2-Epoch {epoch:02d}] Val Loss: {val_loss:.4f}, \"\n",
        "                  f\"Top-1: {val_top1:.4f}, Top-5: {val_top5:.4f}\")\n"
      ],
      "metadata": {
        "id": "mQoM-nuc_Zi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DGAMModel(agc, backbone, num_classes=NUM_CLASSES)\n",
        "train_dgam_switching(model, train_loader, val_loader, device)\n"
      ],
      "metadata": {
        "id": "-4zN0e_p-dtH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}