{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12277203,"sourceType":"datasetVersion","datasetId":7736838}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch-geometric\n!pip install -q torch-geometric-temporal\n!pip install -q mediapipe==0.10.9","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nimport mediapipe as mp\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric.data import Data\nfrom torch_geometric_temporal.nn.attention import STConv\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SkeletonExtractor:\n    def __init__(self):\n        self.mp_pose = mp.solutions.pose\n        self.pose = self.mp_pose.Pose(\n            static_image_mode=False,\n            model_complexity=2,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n\n    def extract_keypoints(self, video_path):\n        cap = cv2.VideoCapture(video_path)\n        keypoints_sequence = []\n\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Convert BGR to RGB\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            results = self.pose.process(rgb_frame)\n\n            if results.pose_landmarks:\n                # Extract 33 keypoints (x, y, z)\n                keypoints = []\n                for landmark in results.pose_landmarks.landmark:\n                    keypoints.extend([landmark.x, landmark.y, landmark.z])\n                keypoints_sequence.append(keypoints)\n            else:\n                # Handle missing pose detection with zero padding\n                keypoints_sequence.append([0.0] * 99)  # 33 * 3 = 99\n\n        cap.release()\n        return np.array(keypoints_sequence)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class STGCNDataPreprocessor:\n    def __init__(self, sequence_length=64, num_joints=33):\n        self.sequence_length = sequence_length\n        self.num_joints = num_joints\n\n    def normalize_keypoints(self, keypoints):\n        # Normalize based on torso/hip center\n        coords = keypoints.reshape(-1, self.num_joints, 3)\n\n        # Center normalization using hip center (MediaPipe indices 23, 24)\n        if coords.shape[0] > 0:\n            hip_center = (coords[:, 23, :] + coords[:, 24, :]) / 2\n            normalized_coords = coords - hip_center[:, np.newaxis, :]\n        else:\n            normalized_coords = coords\n\n        return normalized_coords\n\n    def create_temporal_windows(self, skeleton_data):\n        # Handle sequences shorter than required length\n        if len(skeleton_data) < self.sequence_length:\n            # Pad with zeros or repeat last frame\n            padding_needed = self.sequence_length - len(skeleton_data)\n            if len(skeleton_data) > 0:\n                # Repeat the last frame\n                last_frame = skeleton_data[-1:].repeat(padding_needed, axis=0)\n                skeleton_data = np.concatenate([skeleton_data, last_frame], axis=0)\n            else:\n                # Create zero padding\n                skeleton_data = np.zeros((self.sequence_length, self.num_joints, 3))\n        elif len(skeleton_data) > self.sequence_length:\n            # Sample frames uniformly\n            indices = np.linspace(0, len(skeleton_data)-1, self.sequence_length, dtype=int)\n            skeleton_data = skeleton_data[indices]\n\n        return skeleton_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GraphConstructor:\n    def __init__(self):\n        # MediaPipe skeleton connections\n        self.skeleton_edges = [\n            (11, 12), (11, 13), (13, 15), (12, 14), (14, 16),  # Arms\n            (11, 23), (12, 24), (23, 24),  # Torso\n            (23, 25), (25, 27), (27, 29), (27, 31),  # Left leg\n            (24, 26), (26, 28), (28, 30), (28, 32),  # Right leg\n        ]\n\n    def create_adjacency_matrix(self, num_joints=33):\n        adj_matrix = np.eye(num_joints)\n        for i, j in self.skeleton_edges:\n            adj_matrix[i][j] = 1\n            adj_matrix[j][i] = 1\n        return adj_matrix\n\n    def skeleton_to_graph(self, skeleton_sequence):\n        T, N, C = skeleton_sequence.shape\n        adj_matrix = self.create_adjacency_matrix(N)\n        edge_index = torch.tensor(np.array(np.where(adj_matrix)), dtype=torch.long)\n        # Format for STGCN: (Channel, Time, Joints)\n        x = torch.tensor(skeleton_sequence.transpose(2, 0, 1), dtype=torch.float32)\n        return x, edge_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ARIDDataset(Dataset):\n    def __init__(self,\n                 root_dir,\n                 split_file,\n                 sequence_length=32,\n                 num_joints=33,\n                 precomputed_skeletons=None,\n                 transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Path to ARID dataset root directory\n            split_file (str): Path to split file (split0_train.txt or split0_test.txt)\n                            Format: \"idx class_idx relativepath\"\n                            Example: \"1 0 Drink/Drink1.avi\"\n            sequence_length (int): Number of frames per sequence\n            num_joints (int): Number of keypoints (33 for MediaPipe)\n            precomputed_skeletons (str): Path to precomputed skeleton data (optional)\n            transform: Optional transform to be applied on samples\n        \"\"\"\n        self.root_dir = Path(root_dir)\n        self.clips_dir = self.root_dir / \"clips_v1.5\"\n        self.sequence_length = sequence_length\n        self.num_joints = num_joints\n        self.transform = transform\n\n        # ARID v1.5 has 11 action categories\n        self.class_names = [\n            'drink', 'jump', 'pick', 'pour', 'push',\n            'run', 'sit', 'stand', 'turn', 'walk', 'wave'\n        ]\n        self.num_classes = len(self.class_names)\n\n        # Initialize processing components\n        self.skeleton_extractor = SkeletonExtractor()\n        self.preprocessor = STGCNDataPreprocessor(sequence_length, num_joints)\n        self.graph_constructor = GraphConstructor()\n\n        # Load dataset splits with numeric class labels\n        self.samples = self._load_split_file(split_file)\n\n        print(f\"Loaded {len(self.samples)} samples from {split_file}\")\n        print(f\"Classes: {self.class_names}\")\n        print(f\"Clips directory: {self.clips_dir}\")\n\n        # Load precomputed skeletons if available\n        self.precomputed_skeletons = {}\n        if precomputed_skeletons and os.path.exists(precomputed_skeletons):\n            self._load_precomputed_skeletons(precomputed_skeletons)\n\n    def _load_split_file(self, split_file):\n        \"\"\"Load video paths and labels from split file with format: idx class_idx relativepath\"\"\"\n        samples = []\n\n        with open(split_file, 'r') as f:\n            for line_num, line in enumerate(f, 1):\n                line = line.strip()\n                if line:\n                    # Split by whitespace, handle potential spaces in path\n                    parts = line.split()\n\n                    if len(parts) >= 3:\n                        sample_idx = parts[0]  # Sample index (not used for training)\n                        class_idx = int(parts[1])  # Numeric class index (0-10)\n                        # Join remaining parts in case path contains spaces\n                        relative_path = ' '.join(parts[2:])  # e.g., \"Drink/Drink1.avi\"\n\n                        # Validate class index is within expected range\n                        if 0 <= class_idx < self.num_classes:\n                            # Construct full path: ARID/clips_v1.5/relativepath\n                            full_path = self.clips_dir / relative_path\n\n                            # Check if file exists\n                            if full_path.exists():\n                                samples.append((str(full_path), class_idx, sample_idx))\n                            else:\n                                print(f\"Warning: File not found - {full_path}\")\n                        else:\n                            print(f\"Warning: Invalid class index {class_idx} at line {line_num}\")\n                    else:\n                        print(f\"Warning: Invalid format at line {line_num}: {line}\")\n\n        return samples\n\n    def _load_precomputed_skeletons(self, skeleton_file):\n        \"\"\"Load precomputed skeleton data if available\"\"\"\n        try:\n            with open(skeleton_file, 'r') as f:\n                self.precomputed_skeletons = json.load(f)\n            print(f\"Loaded {len(self.precomputed_skeletons)} precomputed skeletons\")\n        except Exception as e:\n            print(f\"Could not load precomputed skeletons from {skeleton_file}: {e}\")\n\n    def _get_skeleton_data(self, video_path):\n        \"\"\"Get skeleton data either from precomputed or extract from video\"\"\"\n        video_key = str(Path(video_path).name)\n\n        if video_key in self.precomputed_skeletons:\n            return np.array(self.precomputed_skeletons[video_key])\n        else:\n            # Extract skeleton from video\n            return self.skeleton_extractor.extract_keypoints(video_path)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        video_path, class_idx, sample_idx = self.samples[idx]\n\n        try:\n            # Get skeleton data\n            skeleton_data = self._get_skeleton_data(video_path)\n\n            # Preprocess skeleton data\n            if len(skeleton_data) > 0:\n                normalized_skeleton = self.preprocessor.normalize_keypoints(skeleton_data)\n                temporal_skeleton = self.preprocessor.create_temporal_windows(normalized_skeleton)\n            else:\n                # Handle empty skeleton data\n                temporal_skeleton = np.zeros((self.sequence_length, self.num_joints, 3))\n\n            # Convert to graph format\n            x, edge_index = self.graph_constructor.skeleton_to_graph(temporal_skeleton)\n\n            # Apply transforms if any\n            if self.transform:\n                x = self.transform(x)\n\n            return x, edge_index, torch.tensor(class_idx, dtype=torch.long)\n\n        except Exception as e:\n            print(f\"Error processing {video_path}: {e}\")\n            # Return zero tensor in case of error\n            zero_skeleton = np.zeros((self.sequence_length, self.num_joints, 3))\n            x, edge_index = self.graph_constructor.skeleton_to_graph(zero_skeleton)\n            return x, edge_index, torch.tensor(class_idx, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to handle graph data\"\"\"\n    xs, edge_indices, labels = zip(*batch)\n\n    # Stack tensors\n    batch_x = torch.stack(xs, dim=0)  # (batch_size, channels, time, joints)\n    batch_labels = torch.stack(labels, dim=0)\n\n    # Edge indices are the same for all samples (same graph structure)\n    batch_edge_index = edge_indices[0]\n\n    return batch_x, batch_edge_index, batch_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataloaders(dataset_root, train_split_file, test_split_file,\n                      batch_size=8, num_workers=2, sequence_length=32,\n                      precomputed_skeletons=None):\n    \"\"\"\n    Create train and test dataloaders for ARID dataset with numeric class labels\n    \"\"\"\n\n    # Create datasets\n    train_dataset = ARIDDataset(\n        root_dir=dataset_root,\n        split_file=train_split_file,\n        sequence_length=sequence_length,\n        precomputed_skeletons=precomputed_skeletons\n    )\n\n    test_dataset = ARIDDataset(\n        root_dir=dataset_root,\n        split_file=test_split_file,\n        sequence_length=sequence_length,\n        precomputed_skeletons=precomputed_skeletons\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    num_classes = train_dataset.num_classes\n    class_names = train_dataset.class_names\n\n    return train_loader, test_loader, num_classes, class_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ARIDSTGCNClassifier(torch.nn.Module):\n    def __init__(self, num_nodes=33, in_channels=3, hidden_channels=64,\n                 out_channels=64, num_classes=11, kernel_size=3, K=3):\n        super(ARIDSTGCNClassifier, self).__init__()\n\n        # ST-Conv blocks\n        self.stconv1 = STConv(num_nodes, in_channels, hidden_channels,\n                             out_channels, kernel_size, K)\n        self.stconv2 = STConv(num_nodes, out_channels, hidden_channels,\n                             out_channels, kernel_size, K)\n\n        # Classification head\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(out_channels * num_nodes, 256),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x, edge_index, edge_weight=None):\n        # x shape: (batch_size, channels, time_steps, num_nodes)\n        x = F.relu(self.stconv1(x, edge_index, edge_weight))\n        x = F.relu(self.stconv2(x, edge_index, edge_weight))\n\n        # Global average pooling over time dimension\n        x = torch.mean(x, dim=2)  # (batch_size, channels, num_nodes)\n\n        # Flatten and classify\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, data_loader, device):\n    \"\"\"\n    Evaluate classification accuracy of model on data_loader.\n    Returns accuracy as a percentage.\n    \"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, edge_index, labels in data_loader:\n            x = x.to(device)\n            edge_index = edge_index.to(device)\n            labels = labels.to(device)\n            outputs = model(x, edge_index)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    return 100.0 * correct / total","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=100, lr=0.001):\n    \"\"\"\n    Train the STGCN model on ARID dataset\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n\n        for batch_idx, (x, edge_index, labels) in enumerate(train_loader):\n            x, edge_index, labels = x.to(device), edge_index.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(x, edge_index)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        scheduler.step()\n\n        # Validation phase\n        val_acc = evaluate_model(model, val_loader, device)\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), 'best_arid_stgcn.pth')\n\n        print(f'Epoch {epoch+1}/{num_epochs}: '\n              f'Train Loss: {train_loss/len(train_loader):.4f}, '\n              f'Train Acc: {100.*correct/total:.2f}%, '\n              f'Val Acc: {val_acc:.2f}%')\n\n    print(f'Best validation accuracy: {best_acc:.2f}%')\n    return best_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def precompute_skeletons(dataset_root, split_files, output_file):\n    \"\"\"\n    **MAIN SKELETON PRECOMPUTATION FUNCTION**\n    Precompute and save skeleton data for all videos in the dataset\n\n    Args:\n        dataset_root (str): Path to ARID dataset root directory\n        split_files (list): List of split file paths to process\n        output_file (str): Path to output JSON file for skeleton data\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"STARTING SKELETON PRECOMPUTATION\")\n    print(\"=\" * 60)\n\n    extractor = SkeletonExtractor()\n    skeleton_data = {}\n\n    # Process each split file\n    for split_file in split_files:\n        print(f\"\\nProcessing split file: {split_file}\")\n\n        # Create temporary dataset to get video paths\n        temp_dataset = ARIDDataset(dataset_root, split_file)\n\n        for idx, (video_path, class_idx, sample_idx) in enumerate(tqdm(temp_dataset.samples, desc=\"Processing videos\", unit=\"video\")):\n            video_key = Path(video_path).name\n\n            if video_key not in skeleton_data:\n                try:\n                    skeletons = extractor.extract_keypoints(video_path)\n                    skeleton_data[video_key] = skeletons.tolist()\n                except Exception as e:\n                    # Store empty data for failed extractions\n                    skeleton_data[video_key] = []\n\n    # Save to JSON file\n    print(f\"\\nSaving {len(skeleton_data)} skeleton sequences to {output_file}\")\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n    with open(output_file, 'w') as f:\n        json.dump(skeleton_data, f, indent=2)\n\n    print(\"=\" * 60)\n    print(\"SKELETON PRECOMPUTATION COMPLETED\")\n    print(f\"Total videos processed: {len(skeleton_data)}\")\n    print(f\"Saved to: {output_file}\")\n    print(\"=\" * 60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Configuration\n    DATASET_ROOT = \"/kaggle/input/aridzip\"\n    TRAIN_SPLIT = \"/kaggle/input/aridzip/list_cvt/split_0/split0_train.txt\"\n    TEST_SPLIT = \"/kaggle/input/aridzip/list_cvt/split_0/split0_test.txt\"\n    PRECOMPUTED_SKELETON_FILE = \"/kaggle/working/arid_precomputed_skeletons.json\"\n\n    BATCH_SIZE = 8\n    NUM_WORKERS = 2\n    SEQUENCE_LENGTH = 32\n    NUM_EPOCHS = 100\n    LEARNING_RATE = 0.001\n\n    # **STEP 1: PRECOMPUTE SKELETONS (RUN ONCE)**\n    print(\"Step 1: Checking for precomputed skeleton data...\")\n\n    if not os.path.exists(PRECOMPUTED_SKELETON_FILE):\n        print(\"Precomputed skeleton file not found. Starting skeleton extraction...\")\n        print(\"WARNING: This process may take several hours for the full ARID dataset!\")\n\n        # Precompute skeletons for both train and test splits\n        precompute_skeletons(\n            dataset_root=DATASET_ROOT,\n            split_files=[TRAIN_SPLIT, TEST_SPLIT],\n            output_file=PRECOMPUTED_SKELETON_FILE\n        )\n        print(\" Skeleton precomputation completed!\")\n    else:\n        print(f\"Found precomputed skeleton file: {PRECOMPUTED_SKELETON_FILE}\")\n\n    # **STEP 3: CREATE DATALOADERS WITH PRECOMPUTED SKELETONS**\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STEP 3: CREATING DATALOADERS WITH PRECOMPUTED SKELETONS\")\n    print(\"=\" * 60)\n\n    train_loader, test_loader, num_classes, class_names = create_dataloaders(\n        dataset_root=DATASET_ROOT,\n        train_split_file=TRAIN_SPLIT,\n        test_split_file=TEST_SPLIT,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        sequence_length=SEQUENCE_LENGTH,\n        precomputed_skeletons=PRECOMPUTED_SKELETON_FILE  # **KEY INTEGRATION**\n    )\n\n    print(f\"Number of classes: {num_classes}\")\n    print(f\"Class names: {class_names}\")\n    print(f\"Train samples: {len(train_loader.dataset)}\")\n    print(f\"Test samples: {len(test_loader.dataset)}\")\n\n    # **STEP 4: TEST DATALOADER**\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STEP 4: TESTING DATALOADER\")\n    print(\"=\" * 60)\n\n    for batch_idx, (x, edge_index, labels) in enumerate(train_loader):\n        print(f\"Batch {batch_idx + 1}:\")\n        print(f\"  Input shape: {x.shape}\")  # (batch_size, channels, time, joints)\n        print(f\"  Edge index shape: {edge_index.shape}\")\n        print(f\"  Labels shape: {labels.shape}\")\n        print(f\"  Labels (indices): {labels}\")\n        print(f\"  Labels (names): {[class_names[l.item()] for l in labels]}\")\n\n        if batch_idx == 2:  # Only show first 3 batches\n            break\n\n    # **STEP 5: CREATE AND TRAIN MODEL**\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STEP 5: CREATING AND TRAINING MODEL\")\n    print(\"=\" * 60)\n\n    model = ARIDSTGCNClassifier(\n        num_nodes=33,\n        in_channels=3,\n        hidden_channels=64,\n        out_channels=64,\n        num_classes=num_classes,\n        kernel_size=3,\n        K=3\n    )\n\n    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n\n    # Train the model\n    best_accuracy = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=test_loader,  # Using test as validation for simplicity\n        num_epochs=NUM_EPOCHS,\n        lr=LEARNING_RATE\n    )\n\n    print(f\"\\n Training completed! Best accuracy: {best_accuracy:.2f}%\")\n    print(f\"Model saved as: best_arid_stgcn.pth\")\n    print(f\"Precomputed skeletons saved as: {PRECOMPUTED_SKELETON_FILE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}